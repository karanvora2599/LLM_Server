{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"summary\": {\n",
      "        \"text\": \"Fast language models have revolutionized the field of natural language processing (NLP) by enabling efficient and accurate processing of vast amounts of text data. They have numerous applications in various industries and have become a crucial component of many AI-powered systems.\",\n",
      "        \"importance\": \"Fast language models are important because they can process and generate human-like language at high speeds, making them useful for tasks that require quick responses or decisions.\"\n",
      "    },\n",
      "    \"benefits\": [\n",
      "        {\n",
      "            \"name\": \"Improved Accuracy\",\n",
      "            \"description\": \"Fast language models are trained on large datasets and can generate more accurate results than traditional language models.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Faster Processing\",\n",
      "            \"description\": \"They can process large amounts of text data quickly, making them suitable for real-time applications.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Enhanced User Experience\",\n",
      "            \"description\": \"Fast language models can provide quick responses to user queries, leading to a better user experience.\"\n",
      "        }\n",
      "    ],\n",
      "    \"applications\": [\n",
      "        {\n",
      "            \"name\": \"Chatbots\",\n",
      "            \"description\": \"Fast language models are used in chatbots to provide quick and accurate responses to user queries.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Language Translation\",\n",
      "            \"description\": \"They are used in language translation applications to quickly translate text and maintain accuracy.\"\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Question Answering\",\n",
      "            \"description\": \"Fast language models are used in question answering systems to quickly identify and retrieve answers from large text datasets.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"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\",\n",
    ")\n",
    "\n",
    "# Modify the prompt to request JSON output\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Explain the importance of fast language models. \"\n",
    "                \"Provide the answer in JSON format with keys 'summary', 'benefits', and 'applications'.\"\n",
    "            ),\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "content = chat_completion.choices[0].message.content\n",
    "\n",
    "# Use regex to extract JSON content in case there is extra text\n",
    "json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "\n",
    "if json_match:\n",
    "    json_content = json_match.group()\n",
    "    try:\n",
    "        data = json.loads(json_content)\n",
    "        print(json.dumps(data, indent=4))\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Failed to parse JSON:\", e)\n",
    "        print(\"Content was:\", json_content)\n",
    "else:\n",
    "    print(\"No JSON content found in the response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Inference Default\n",
    "\n",
    "import requests\n",
    "\n",
    "api_key = \"ljoWc7l91mdURBRNTNPlZQlGpV5OMo\"  # Replace with your actual API key\n",
    "\n",
    "url = \"http://localhost:8000/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": api_key,\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}\n",
    "    ],\n",
    "    \"model\": \"llama3.1-8b-8192\",\n",
    "    \"service_name\": \"cerebras\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(result[\"content\"])\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM Inference Custom Parameters\n",
    "\n",
    "import requests\n",
    "\n",
    "api_key = \"ljoWc7l91mdURBRNTNPlZQlGpV5OMo\"  # Replace with your actual API key\n",
    "\n",
    "url = \"http://localhost:8000/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": api_key,\n",
    "}\n",
    "\n",
    "data = {\n",
    "  \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}\n",
    "    ],\n",
    "  \"model\": \"llama3.1-8b\",\n",
    "  \"service_name\": \"cerebras\",\n",
    "  \"temperature\": 0.7,\n",
    "  \"max_tokens\": 500,\n",
    "  \"top_p\": 0.9,\n",
    "  \"stop\": [\"\\n\"]\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(result[\"content\"])\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"readability\": 8,\n",
      "  \"conciseness\": 6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "api_key = \"ljoWc7l91mdURBRNTNPlZQlGpV5OMo\"  # Replace with your actual API key\n",
    "url = \"http://localhost:8000/chat/completions\"  # Adjust the URL if necessary\n",
    "\n",
    "# System prompt\n",
    "system_prompt = \"\"\"\n",
    "You're evaluating writing style in text.\n",
    "Your evaluations must always be in JSON format. Here is an example JSON response:\n",
    "\n",
    "{ \"readability\": 4, \"conciseness\": 2 }\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# User prompt\n",
    "tweet = \"This is a sample text that needs evaluation.\"\n",
    "prompt = f\"\"\"\n",
    "Evaluate the text:\n",
    "\n",
    "{tweet}\n",
    "\n",
    "You're evaluating the readability and conciseness with values from 0 (extremely bad) to 10 (extremely good).\n",
    "\"\"\"\n",
    "\n",
    "# Messages\n",
    "messages = [    {\"role\": \"system\", \"content\": system_prompt},    {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "# Request data\n",
    "data = {\n",
    "    \"messages\": messages,\n",
    "    \"model\": \"llama3.1-8b\",  # Replace with your desired model\n",
    "    \"service_name\": \"cerebras\",     # Replace with your desired service\n",
    "    \"temperature\": 0.0,         # Low temperature for deterministic output\n",
    "    \"max_tokens\": 100,          # Adjust as needed\n",
    "    \"top_p\": 1.0,\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": api_key,\n",
    "}\n",
    "\n",
    "# Make the API call\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(result[\"content\"])\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ChatOllama(model=\"mistral:7b-instruct-v0.2-q4_1\")\n",
    "parser = StrOutputParser()\n",
    "output = 'News_data_Company_SP500_3_Sentiment_Score.xlsx'\n",
    "Question = \"\"\"\n",
    "    In this format respond to the news aricle given in the context interpret it and classify and respond in this JSON format only. No other things and extra explaination or interpretation.\n",
    "    JSON = {\n",
    "        Sentiment: Positive/Negative/Neutral,\n",
    "        Corruption: Yes/No/Possible,\n",
    "        Fraud: Yes/No/Possible,\n",
    "        Type: (Type of news article),\n",
    "        Explanation: (Write Explanation Here),\n",
    "    }\n",
    "    Return just the JSON No matter what. Strictly the formatted JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_json(json_data):\n",
    "    try:\n",
    "        json_object = json.loads(json_data)\n",
    "    except ValueError as e:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def fix_json(json_data):\n",
    "    try:\n",
    "        if not json_data.strip().endswith('}'):\n",
    "            json_data += '}'\n",
    "        return json_data\n",
    "    except Exception as e:\n",
    "        return json_data\n",
    "    \n",
    "def perform_sentiment_analysis(text, model, parser, Question):\n",
    "    template = \"\"\"\n",
    "    This is a news article from Thompson Reuters. Use this news article and respond only in the format asked in the question nothing extra. Do not respond with anything else just the JSON format.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | parser\n",
    "    response = chain.invoke({\n",
    "        \"context\": text,\n",
    "        \"question\": Question\n",
    "    })\n",
    "    if not is_valid_json(response):\n",
    "        fixed_response = fix_json(response)\n",
    "        if is_valid_json(fixed_response):\n",
    "            return json.loads(fixed_response)\n",
    "        else:\n",
    "            response = {\n",
    "                'Sentiment': None,\n",
    "                'Corruption': None,\n",
    "                'Fraud': None,\n",
    "                'Type': None,\n",
    "                'Explanation': \"Error in Analysis\"\n",
    "            }\n",
    "            return(response)\n",
    "    else:\n",
    "        return json.loads(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
