{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import threading\n",
    "import json\n",
    "from fastapi import FastAPI, HTTPException, Header\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from groq import Groq\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize the master API client with your API key\n",
    "master_client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Thread lock for synchronizing access to api_keys.json\n",
    "api_keys_lock = threading.Lock()\n",
    "\n",
    "# Define the request and response models\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    messages: List[Message]\n",
    "    model: str\n",
    "\n",
    "class ChatCompletionResponse(BaseModel):\n",
    "    content: str\n",
    "\n",
    "def generate_api_key(length=30):\n",
    "    \"\"\"Generates a random API key of the given length.\"\"\"\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    api_key = ''.join(random.choice(characters) for _ in range(length))\n",
    "    return api_key\n",
    "\n",
    "def load_api_keys():\n",
    "    \"\"\"Loads API keys from the JSON file.\"\"\"\n",
    "    with api_keys_lock:\n",
    "        if not os.path.exists('api_keys.json'):\n",
    "            return {}\n",
    "        with open('api_keys.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "def save_api_keys(api_keys):\n",
    "    \"\"\"Saves API keys to the JSON file.\"\"\"\n",
    "    with api_keys_lock:\n",
    "        with open('api_keys.json', 'w') as f:\n",
    "            json.dump(api_keys, f, indent=4)\n",
    "\n",
    "def add_api_key(client_name):\n",
    "    \"\"\"Generates a new API key, adds it to api_keys.json, and returns the key.\"\"\"\n",
    "    api_keys = load_api_keys()\n",
    "    new_key = generate_api_key()\n",
    "    api_keys[new_key] = client_name\n",
    "    save_api_keys(api_keys)\n",
    "    return new_key\n",
    "\n",
    "@app.post(\"/generate_api_key\")\n",
    "async def generate_api_key_endpoint(client_name: str):\n",
    "    \"\"\"API endpoint to generate a new API key.\"\"\"\n",
    "    new_key = add_api_key(client_name)\n",
    "    return {\"api_key\": new_key}\n",
    "\n",
    "@app.post(\"/chat/completions\", response_model=ChatCompletionResponse)\n",
    "async def chat_completions(\n",
    "    request: ChatCompletionRequest,\n",
    "    api_key: Optional[str] = Header(None),\n",
    "):\n",
    "    # Load the API keys\n",
    "    api_keys = load_api_keys()\n",
    "\n",
    "    # Authenticate the client using the wrapper's API key\n",
    "    if api_key not in api_keys:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n",
    "\n",
    "    try:\n",
    "        # Forward the request to the master API\n",
    "        chat_completion = master_client.chat.completions.create(\n",
    "            messages=[message.dict() for message in request.messages],\n",
    "            model=request.model,\n",
    "        )\n",
    "\n",
    "        # Extract the response content\n",
    "        content = chat_completion.choices[0].message.content\n",
    "\n",
    "        # Return the response to the client\n",
    "        return ChatCompletionResponse(content=content)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fastapi import FastAPI, HTTPException, Header\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from groq import Groq\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the request and response models\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    messages: List[Message]\n",
    "    model: str\n",
    "    service_name: str  # Service to use: 'cerebras' or 'groq'\n",
    "\n",
    "class ChatCompletionResponse(BaseModel):\n",
    "    content: str\n",
    "\n",
    "# API keys for the services\n",
    "SERVICE_API_KEYS = {\n",
    "    \"cerebras\": os.environ.get(\"CEREBRAS_API_KEY\"),\n",
    "    \"groq\": os.environ.get(\"GROQ_API_KEY\"),\n",
    "}\n",
    "\n",
    "# Ensure all necessary API keys are provided\n",
    "for service, api_key in SERVICE_API_KEYS.items():\n",
    "    if not api_key:\n",
    "        raise Exception(f\"API key for {service} not set in environment variables.\")\n",
    "\n",
    "def get_client(service_name: str):\n",
    "    \"\"\"Get the appropriate client based on the service name.\"\"\"\n",
    "    service_name = service_name.lower()\n",
    "    if service_name == \"cerebras\":\n",
    "        return Cerebras(api_key=SERVICE_API_KEYS[\"cerebras\"])\n",
    "    elif service_name == \"groq\":\n",
    "        return Groq(api_key=SERVICE_API_KEYS[\"groq\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported service_name: {service_name}\")\n",
    "\n",
    "@app.post(\"/chat/completions\", response_model=ChatCompletionResponse)\n",
    "async def chat_completions(\n",
    "    request: ChatCompletionRequest,\n",
    "    api_key: Optional[str] = Header(None),\n",
    "):\n",
    "    service_name = request.service_name.lower()\n",
    "\n",
    "    try:\n",
    "        client = get_client(service_name)\n",
    "        messages = [message.dict() for message in request.messages]\n",
    "\n",
    "        if service_name == \"cerebras\":\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=request.model,\n",
    "            )\n",
    "            content = chat_completion  # Adjust as per actual response\n",
    "\n",
    "        elif service_name == \"groq\":\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=request.model,\n",
    "            )\n",
    "            content = chat_completion.choices[0].message.content\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported service_name: {service_name}\")\n",
    "\n",
    "        return ChatCompletionResponse(content=content)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        raise HTTPException(status_code=400, detail=str(ve))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uvicorn wrapper_service:app --reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your API Key: ljoWc7l91mdURBRNTNPlZQlGpV5OMo\n"
     ]
    }
   ],
   "source": [
    "#New Key\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"http://localhost:8000/generate_api_key\"\n",
    "data = {\n",
    "    \"name\": \"John Doe\",\n",
    "    \"email\": \"john.doe@example.com\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    api_key = response.json()[\"api_key\"]\n",
    "    print(f\"Your API Key: {api_key}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Fast language models, also known as efficient language models or accelerated language models, have gained significant attention in recent years. These models are designed to achieve better performance while reducing computational complexity and memory requirements, making them more feasible for deployment on various devices and applications. The importance of fast language models can be summarized as follows:\n",
      "\n",
      "1. **Scalability**: Fast language models enable the development of language-based applications that can handle large-scale data, support a large number of users, and function across various devices and platforms. This is particularly crucial for applications like chatbots, virtual assistants, and language translation services.\n",
      "2. **Real-time Processing**: Fast language models allow for real-time processing of natural language text, enabling applications to respond quickly to user inputs. This is essential for applications that require immediate feedback, such as customer support, language translation, and content generation.\n",
      "3. **Low Latency**: Fast language models reduce latency, which is critical for applications that require rapid processing, such as real-time language translation, speech recognition, and natural language processing (NLP) tasks.\n",
      "4. **Energy Efficiency**: Fast language models often require less computational power and energy consumption, making them suitable for deployment on devices with limited resources, such as mobile devices, embedded systems, and edge devices.\n",
      "5. **Edge Computing**: Fast language models enable edge computing, which is a computational paradigm that offloads processing from central servers to edge devices, reducing latency and improving real-time processing capabilities.\n",
      "6. **Multi-Modal Processing**: Fast language models can be designed to process multiple modalities of language input, such as text, speech, and images, enabling more comprehensive language understanding and processing.\n",
      "7. **Explainability and Transparency**: Fast language models can be designed to provide better explainability and transparency, making it easier to understand the decision-making process behind language processing tasks.\n",
      "8. **Advances in NLP Research**: The development of fast language models drives progress in NLP research, encouraging the exploration of new architectures, techniques, and algorithms, which can lead to further improvements in language understanding and processing capabilities.\n",
      "9. **Practical Deployability**: Fast language models can be more easily deployed in practical applications, as they require less computational resources and infrastructure, making them more accessible to a broader range of organizations and individuals.\n",
      "10. **Economic Impact**: Fast language models can drive economic growth, innovation, and job creation, particularly in industries that rely heavily on language-based applications, such as customer service, language translation, and content creation.\n",
      "\n",
      "In summary, fast language models are essential for enabling the development of efficient, scalable, and practical language-based applications that can process real-time language input, provide low latency, and support multiple modalities. These benefits make fast language models crucial for advancing the field of NLP and driving innovation in various industries.\n"
     ]
    }
   ],
   "source": [
    "#LLM Inference\n",
    "\n",
    "import requests\n",
    "\n",
    "api_key = \"ljoWc7l91mdURBRNTNPlZQlGpV5OMo\"  # Replace with your actual API key\n",
    "\n",
    "url = \"http://localhost:8000/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": api_key,\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}\n",
    "    ],\n",
    "    \"model\": \"llama3-8b-8192\",\n",
    "    \"service_name\": \"groq\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"Response:\", result[\"content\"])\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import json\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI, HTTPException, Header, Depends\n",
    "from pydantic import BaseModel, EmailStr\n",
    "from typing import List\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from groq import Groq\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, func\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Thread lock for synchronizing access to api_keys.json\n",
    "api_keys_lock = threading.Lock()\n",
    "\n",
    "# Database setup\n",
    "DATABASE_URL = \"postgresql://username:password@host:port/database_name\"\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the ChatLog model\n",
    "class ChatLog(Base):\n",
    "    __tablename__ = 'chat_logs'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    timestamp = Column(DateTime(timezone=True), server_default=func.now())\n",
    "    client_name = Column(String(100))\n",
    "    client_email = Column(String(100))\n",
    "    service_name = Column(String(50))\n",
    "    model_name = Column(String(100))\n",
    "    client_message = Column(Text)\n",
    "    content_in_response = Column(Text)\n",
    "\n",
    "# Create the tables in the database\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "# Master API keys\n",
    "MASTER_SERVICE_API_KEYS = {\n",
    "    \"cerebras\": os.environ.get(\"CEREBRAS_API_KEY\"),\n",
    "    \"groq\": os.environ.get(\"GROQ_API_KEY\"),\n",
    "    # Add more services as needed\n",
    "}\n",
    "\n",
    "for service, api_key in MASTER_SERVICE_API_KEYS.items():\n",
    "    if not api_key:\n",
    "        raise Exception(f\"Master API key for {service} is not set in environment variables.\")\n",
    "\n",
    "# Models\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    messages: List[Message]\n",
    "    model: str\n",
    "    service_name: str\n",
    "\n",
    "class ChatCompletionResponse(BaseModel):\n",
    "    content: str\n",
    "\n",
    "class GenerateApiKeyRequest(BaseModel):\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "\n",
    "def generate_api_key(length=30):\n",
    "    import random\n",
    "    import string\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(characters) for _ in range(length))\n",
    "\n",
    "def load_api_keys():\n",
    "    with api_keys_lock:\n",
    "        if not os.path.exists('api_keys.json'):\n",
    "            return {}\n",
    "        with open('api_keys.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "def save_api_keys(api_keys):\n",
    "    with api_keys_lock:\n",
    "        with open('api_keys.json', 'w') as f:\n",
    "            json.dump(api_keys, f, indent=4)\n",
    "\n",
    "def add_api_key(client_name, client_email):\n",
    "    api_keys = load_api_keys()\n",
    "    new_key = generate_api_key()\n",
    "    api_keys[new_key] = {\n",
    "        \"name\": client_name,\n",
    "        \"email\": client_email\n",
    "    }\n",
    "    save_api_keys(api_keys)\n",
    "    return new_key\n",
    "\n",
    "def authenticate_client(api_key: str = Header(...)):\n",
    "    api_keys = load_api_keys()\n",
    "    if api_key not in api_keys:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n",
    "    return api_key\n",
    "\n",
    "def get_master_client(service_name: str):\n",
    "    service_name = service_name.lower()\n",
    "    if service_name == \"cerebras\":\n",
    "        api_key = MASTER_SERVICE_API_KEYS[\"cerebras\"]\n",
    "        return Cerebras(api_key=api_key)\n",
    "    elif service_name == \"groq\":\n",
    "        api_key = MASTER_SERVICE_API_KEYS[\"groq\"]\n",
    "        return Groq(api_key=api_key)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported service_name: {service_name}\")\n",
    "\n",
    "def log_to_database(log_entry):\n",
    "    session = SessionLocal()\n",
    "    try:\n",
    "        session.add(log_entry)\n",
    "        session.commit()\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"Error logging to database: {e}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "@app.post(\"/generate_api_key\")\n",
    "async def generate_api_key_endpoint(request: GenerateApiKeyRequest):\n",
    "    client_name = request.name\n",
    "    client_email = request.email\n",
    "\n",
    "    api_keys = load_api_keys()\n",
    "    for key_info in api_keys.values():\n",
    "        if key_info['email'].lower() == client_email.lower():\n",
    "            raise HTTPException(status_code=400, detail=\"An API key has already been generated for this email.\")\n",
    "\n",
    "    new_key = add_api_key(client_name, client_email)\n",
    "    return {\"api_key\": new_key}\n",
    "\n",
    "@app.post(\"/chat/completions\", response_model=ChatCompletionResponse)\n",
    "async def chat_completions(\n",
    "    request: ChatCompletionRequest,\n",
    "    api_key: str = Depends(authenticate_client),\n",
    "):\n",
    "    service_name = request.service_name.lower()\n",
    "    model_name = request.model\n",
    "\n",
    "    try:\n",
    "        client = get_master_client(service_name)\n",
    "        messages = [message.dict() for message in request.messages]\n",
    "\n",
    "        # Retrieve client information\n",
    "        api_keys = load_api_keys()\n",
    "        client_info = api_keys[api_key]\n",
    "        client_name = client_info['name']\n",
    "        client_email = client_info['email']\n",
    "\n",
    "        # Extract client message\n",
    "        client_message = ' '.join(\n",
    "            [msg.content for msg in request.messages if msg.role == 'user']\n",
    "        )\n",
    "\n",
    "        # Process the request\n",
    "        if service_name == \"cerebras\":\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model_name,\n",
    "            )\n",
    "            content_in_response = chat_completion.get('content', '')\n",
    "\n",
    "        elif service_name == \"groq\":\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model_name,\n",
    "            )\n",
    "            content_in_response = chat_completion.choices[0].message.content\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported service_name: {service_name}\")\n",
    "\n",
    "        # Log the data to the database\n",
    "        log_entry = ChatLog(\n",
    "            client_name=client_name,\n",
    "            client_email=client_email,\n",
    "            service_name=service_name,\n",
    "            model_name=model_name,\n",
    "            client_message=client_message.strip(),\n",
    "            content_in_response=content_in_response.strip()\n",
    "        )\n",
    "        log_to_database(log_entry)\n",
    "\n",
    "        return ChatCompletionResponse(content=content_in_response)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        raise HTTPException(status_code=400, detail=str(ve))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
