{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import threading\n",
    "import json\n",
    "from fastapi import FastAPI, HTTPException, Header\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from groq import Groq\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Initialize the master API client with your API key\n",
    "master_client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "# Thread lock for synchronizing access to api_keys.json\n",
    "api_keys_lock = threading.Lock()\n",
    "\n",
    "# Define the request and response models\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    messages: List[Message]\n",
    "    model: str\n",
    "\n",
    "class ChatCompletionResponse(BaseModel):\n",
    "    content: str\n",
    "\n",
    "def generate_api_key(length=30):\n",
    "    \"\"\"Generates a random API key of the given length.\"\"\"\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    api_key = ''.join(random.choice(characters) for _ in range(length))\n",
    "    return api_key\n",
    "\n",
    "def load_api_keys():\n",
    "    \"\"\"Loads API keys from the JSON file.\"\"\"\n",
    "    with api_keys_lock:\n",
    "        if not os.path.exists('api_keys.json'):\n",
    "            return {}\n",
    "        with open('api_keys.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "def save_api_keys(api_keys):\n",
    "    \"\"\"Saves API keys to the JSON file.\"\"\"\n",
    "    with api_keys_lock:\n",
    "        with open('api_keys.json', 'w') as f:\n",
    "            json.dump(api_keys, f, indent=4)\n",
    "\n",
    "def add_api_key(client_name):\n",
    "    \"\"\"Generates a new API key, adds it to api_keys.json, and returns the key.\"\"\"\n",
    "    api_keys = load_api_keys()\n",
    "    new_key = generate_api_key()\n",
    "    api_keys[new_key] = client_name\n",
    "    save_api_keys(api_keys)\n",
    "    return new_key\n",
    "\n",
    "@app.post(\"/generate_api_key\")\n",
    "async def generate_api_key_endpoint(client_name: str):\n",
    "    \"\"\"API endpoint to generate a new API key.\"\"\"\n",
    "    new_key = add_api_key(client_name)\n",
    "    return {\"api_key\": new_key}\n",
    "\n",
    "@app.post(\"/chat/completions\", response_model=ChatCompletionResponse)\n",
    "async def chat_completions(\n",
    "    request: ChatCompletionRequest,\n",
    "    api_key: Optional[str] = Header(None),\n",
    "):\n",
    "    # Load the API keys\n",
    "    api_keys = load_api_keys()\n",
    "\n",
    "    # Authenticate the client using the wrapper's API key\n",
    "    if api_key not in api_keys:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n",
    "\n",
    "    try:\n",
    "        # Forward the request to the master API\n",
    "        chat_completion = master_client.chat.completions.create(\n",
    "            messages=[message.dict() for message in request.messages],\n",
    "            model=request.model,\n",
    "        )\n",
    "\n",
    "        # Extract the response content\n",
    "        content = chat_completion.choices[0].message.content\n",
    "\n",
    "        # Return the response to the client\n",
    "        return ChatCompletionResponse(content=content)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from fastapi import FastAPI, HTTPException, Header\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from groq import Groq\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Define the request and response models\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    messages: List[Message]\n",
    "    model: str\n",
    "    service_name: str  # Service to use: 'cerebras' or 'groq'\n",
    "\n",
    "class ChatCompletionResponse(BaseModel):\n",
    "    content: str\n",
    "\n",
    "# API keys for the services\n",
    "SERVICE_API_KEYS = {\n",
    "    \"cerebras\": os.environ.get(\"CEREBRAS_API_KEY\"),\n",
    "    \"groq\": os.environ.get(\"GROQ_API_KEY\"),\n",
    "}\n",
    "\n",
    "# Ensure all necessary API keys are provided\n",
    "for service, api_key in SERVICE_API_KEYS.items():\n",
    "    if not api_key:\n",
    "        raise Exception(f\"API key for {service} not set in environment variables.\")\n",
    "\n",
    "def get_client(service_name: str):\n",
    "    \"\"\"Get the appropriate client based on the service name.\"\"\"\n",
    "    service_name = service_name.lower()\n",
    "    if service_name == \"cerebras\":\n",
    "        return Cerebras(api_key=SERVICE_API_KEYS[\"cerebras\"])\n",
    "    elif service_name == \"groq\":\n",
    "        return Groq(api_key=SERVICE_API_KEYS[\"groq\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported service_name: {service_name}\")\n",
    "\n",
    "@app.post(\"/chat/completions\", response_model=ChatCompletionResponse)\n",
    "async def chat_completions(\n",
    "    request: ChatCompletionRequest,\n",
    "    api_key: Optional[str] = Header(None),\n",
    "):\n",
    "    service_name = request.service_name.lower()\n",
    "\n",
    "    try:\n",
    "        client = get_client(service_name)\n",
    "        messages = [message.dict() for message in request.messages]\n",
    "\n",
    "        if service_name == \"cerebras\":\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=request.model,\n",
    "            )\n",
    "            content = chat_completion  # Adjust as per actual response\n",
    "\n",
    "        elif service_name == \"groq\":\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=request.model,\n",
    "            )\n",
    "            content = chat_completion.choices[0].message.content\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported service_name: {service_name}\")\n",
    "\n",
    "        return ChatCompletionResponse(content=content)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        raise HTTPException(status_code=400, detail=str(ve))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uvicorn wrapper_service:app --reload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your API Key: ljoWc7l91mdURBRNTNPlZQlGpV5OMo\n"
     ]
    }
   ],
   "source": [
    "#New Key\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"http://localhost:8000/generate_api_key\"\n",
    "data = {\n",
    "    \"name\": \"John Doe\",\n",
    "    \"email\": \"john.doe@example.com\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    api_key = response.json()[\"api_key\"]\n",
    "    print(f\"Your API Key: {api_key}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error 400: {\"detail\":\"Unsupported service_name: cerebra\"}\n"
     ]
    }
   ],
   "source": [
    "#LLM Inference\n",
    "\n",
    "import requests\n",
    "\n",
    "api_key = \"ljoWc7l91mdURBRNTNPlZQlGpV5OMo\"  # Replace with your actual API key\n",
    "\n",
    "url = \"http://localhost:8000/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": api_key,\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}\n",
    "    ],\n",
    "    \"model\": \"llama3.1-8b-8192\",\n",
    "    \"service_name\": \"cerebras\",\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(result[\"content\"])\n",
    "else:\n",
    "    print(f\"Error {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import json\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI, HTTPException, Header, Depends\n",
    "from pydantic import BaseModel, EmailStr\n",
    "from typing import List\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "from groq import Groq\n",
    "from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime, func\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Thread lock for synchronizing access to api_keys.json\n",
    "api_keys_lock = threading.Lock()\n",
    "\n",
    "# Database setup\n",
    "DATABASE_URL = \"postgresql://username:password@host:port/database_name\"\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define the ChatLog model\n",
    "class ChatLog(Base):\n",
    "    __tablename__ = 'chat_logs'\n",
    "    \n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    timestamp = Column(DateTime(timezone=True), server_default=func.now())\n",
    "    client_name = Column(String(100))\n",
    "    client_email = Column(String(100))\n",
    "    service_name = Column(String(50))\n",
    "    model_name = Column(String(100))\n",
    "    client_message = Column(Text)\n",
    "    content_in_response = Column(Text)\n",
    "\n",
    "# Create the tables in the database\n",
    "Base.metadata.create_all(bind=engine)\n",
    "\n",
    "# Master API keys\n",
    "MASTER_SERVICE_API_KEYS = {\n",
    "    \"cerebras\": os.environ.get(\"CEREBRAS_API_KEY\"),\n",
    "    \"groq\": os.environ.get(\"GROQ_API_KEY\"),\n",
    "    # Add more services as needed\n",
    "}\n",
    "\n",
    "for service, api_key in MASTER_SERVICE_API_KEYS.items():\n",
    "    if not api_key:\n",
    "        raise Exception(f\"Master API key for {service} is not set in environment variables.\")\n",
    "\n",
    "# Models\n",
    "class Message(BaseModel):\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    messages: List[Message]\n",
    "    model: str\n",
    "    service_name: str\n",
    "\n",
    "class ChatCompletionResponse(BaseModel):\n",
    "    content: str\n",
    "\n",
    "class GenerateApiKeyRequest(BaseModel):\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "\n",
    "def generate_api_key(length=30):\n",
    "    import random\n",
    "    import string\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choice(characters) for _ in range(length))\n",
    "\n",
    "def load_api_keys():\n",
    "    with api_keys_lock:\n",
    "        if not os.path.exists('api_keys.json'):\n",
    "            return {}\n",
    "        with open('api_keys.json', 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "def save_api_keys(api_keys):\n",
    "    with api_keys_lock:\n",
    "        with open('api_keys.json', 'w') as f:\n",
    "            json.dump(api_keys, f, indent=4)\n",
    "\n",
    "def add_api_key(client_name, client_email):\n",
    "    api_keys = load_api_keys()\n",
    "    new_key = generate_api_key()\n",
    "    api_keys[new_key] = {\n",
    "        \"name\": client_name,\n",
    "        \"email\": client_email\n",
    "    }\n",
    "    save_api_keys(api_keys)\n",
    "    return new_key\n",
    "\n",
    "def authenticate_client(api_key: str = Header(...)):\n",
    "    api_keys = load_api_keys()\n",
    "    if api_key not in api_keys:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n",
    "    return api_key\n",
    "\n",
    "def get_master_client(service_name: str):\n",
    "    service_name = service_name.lower()\n",
    "    if service_name == \"cerebras\":\n",
    "        api_key = MASTER_SERVICE_API_KEYS[\"cerebras\"]\n",
    "        return Cerebras(api_key=api_key)\n",
    "    elif service_name == \"groq\":\n",
    "        api_key = MASTER_SERVICE_API_KEYS[\"groq\"]\n",
    "        return Groq(api_key=api_key)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported service_name: {service_name}\")\n",
    "\n",
    "def log_to_database(log_entry):\n",
    "    session = SessionLocal()\n",
    "    try:\n",
    "        session.add(log_entry)\n",
    "        session.commit()\n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"Error logging to database: {e}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "@app.post(\"/generate_api_key\")\n",
    "async def generate_api_key_endpoint(request: GenerateApiKeyRequest):\n",
    "    client_name = request.name\n",
    "    client_email = request.email\n",
    "\n",
    "    api_keys = load_api_keys()\n",
    "    for key_info in api_keys.values():\n",
    "        if key_info['email'].lower() == client_email.lower():\n",
    "            raise HTTPException(status_code=400, detail=\"An API key has already been generated for this email.\")\n",
    "\n",
    "    new_key = add_api_key(client_name, client_email)\n",
    "    return {\"api_key\": new_key}\n",
    "\n",
    "@app.post(\"/chat/completions\", response_model=ChatCompletionResponse)\n",
    "async def chat_completions(\n",
    "    request: ChatCompletionRequest,\n",
    "    api_key: str = Depends(authenticate_client),\n",
    "):\n",
    "    service_name = request.service_name.lower()\n",
    "    model_name = request.model\n",
    "\n",
    "    try:\n",
    "        client = get_master_client(service_name)\n",
    "        messages = [message.dict() for message in request.messages]\n",
    "\n",
    "        # Retrieve client information\n",
    "        api_keys = load_api_keys()\n",
    "        client_info = api_keys[api_key]\n",
    "        client_name = client_info['name']\n",
    "        client_email = client_info['email']\n",
    "\n",
    "        # Extract client message\n",
    "        client_message = ' '.join(\n",
    "            [msg.content for msg in request.messages if msg.role == 'user']\n",
    "        )\n",
    "\n",
    "        # Process the request\n",
    "        if service_name == \"cerebras\":\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model_name,\n",
    "            )\n",
    "            content_in_response = chat_completion.get('content', '')\n",
    "\n",
    "        elif service_name == \"groq\":\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                model=model_name,\n",
    "            )\n",
    "            content_in_response = chat_completion.choices[0].message.content\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported service_name: {service_name}\")\n",
    "\n",
    "        # Log the data to the database\n",
    "        log_entry = ChatLog(\n",
    "            client_name=client_name,\n",
    "            client_email=client_email,\n",
    "            service_name=service_name,\n",
    "            model_name=model_name,\n",
    "            client_message=client_message.strip(),\n",
    "            content_in_response=content_in_response.strip()\n",
    "        )\n",
    "        log_to_database(log_entry)\n",
    "\n",
    "        return ChatCompletionResponse(content=content_in_response)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        raise HTTPException(status_code=400, detail=str(ve))\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe: Apple Pie\n",
      "\n",
      "Ingredients:\n",
      "- Flour: 2 1/4 cups cups\n",
      "- Cold Butter: 1 cup cups\n",
      "- Granulated Sugar: 1/2 cup cups\n",
      "- Salt: 1/4 teaspoon teaspoons\n",
      "- Ground Cinnamon: 1/2 teaspoon teaspoons\n",
      "- Ground Nutmeg: 1/4 teaspoon teaspoons\n",
      "- Eggs: 1 whole\n",
      "- Apple Filling: 6-8 cups cups\n",
      "\n",
      "Directions:\n",
      "1. Preheat oven to 375°F (190°C).\n",
      "2. Make the crust: In a large bowl, combine flour, salt, and cold butter. Use a pastry blender or your fingers to work the butter into the flour until the mixture resembles coarse crumbs.\n",
      "3. Add the sugar, cinnamon, and nutmeg to the flour mixture and stir until combined.\n",
      "4. Gradually add ice-cold water, stirring with a fork until the dough comes together in a ball.\n",
      "5. Turn the dough out onto a lightly floured surface and knead a few times until it becomes smooth and pliable.\n",
      "6. Divide the dough in half and shape each half into a disk. Wrap each disk in plastic wrap and refrigerate for at least 30 minutes.\n",
      "7. Make the filling: Peel, core, and slice the apples. In a large bowl, combine the sliced apples, granulated sugar, and cinnamon. Mix until the apples are evenly coated with the sugar and cinnamon.\n",
      "8. Roll out one of the chilled dough disks to a thickness of about 1/8 inch. Place the dough into a 9-inch pie dish and trim the edges to fit.\n",
      "9. Fill the pie crust with the apple filling and dot the top with butter.\n",
      "10. Roll out the second dough disk to a thickness of about 1/8 inch. Use this dough to cover the pie and crimp the edges to seal.\n",
      "11. Cut a few slits in the top crust to allow steam to escape during baking.\n",
      "12. Bake the pie for 45-50 minutes, or until the crust is golden brown and the apples are tender.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from groq import Groq\n",
    "\n",
    "groq = Groq(\n",
    "    api_key=\"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\"\n",
    ")\n",
    "\n",
    "\n",
    "# Data model for LLM to generate\n",
    "class Ingredient(BaseModel):\n",
    "    name: str\n",
    "    quantity: str\n",
    "    quantity_unit: Optional[str]\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    recipe_name: str\n",
    "    ingredients: List[Ingredient]\n",
    "    directions: List[str]\n",
    "\n",
    "\n",
    "def get_recipe(recipe_name: str) -> Recipe:\n",
    "    chat_completion = groq.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a recipe database that outputs recipes in JSON.\\n\"\n",
    "                # Pass the json schema to the model. Pretty printing improves results.\n",
    "                f\" The JSON object must use the schema: {json.dumps(Recipe.model_json_schema(), indent=2)}\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Fetch a recipe for {recipe_name}\",\n",
    "            },\n",
    "        ],\n",
    "        model=\"llama3-8b-8192\",\n",
    "        temperature=0,\n",
    "        # Streaming is not supported in JSON mode\n",
    "        stream=False,\n",
    "        # Enable JSON mode by setting the response format\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return Recipe.model_validate_json(chat_completion.choices[0].message.content)\n",
    "\n",
    "\n",
    "def print_recipe(recipe: Recipe):\n",
    "    print(\"Recipe:\", recipe.recipe_name)\n",
    "\n",
    "    print(\"\\nIngredients:\")\n",
    "    for ingredient in recipe.ingredients:\n",
    "        print(\n",
    "            f\"- {ingredient.name}: {ingredient.quantity} {ingredient.quantity_unit or ''}\"\n",
    "        )\n",
    "    print(\"\\nDirections:\")\n",
    "    for step, direction in enumerate(recipe.directions, start=1):\n",
    "        print(f\"{step}. {direction}\")\n",
    "\n",
    "\n",
    "recipe = get_recipe(\"apple pie\")\n",
    "# print(type(recipe))\n",
    "print_recipe(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-cc4ce3aa-7b44-4e95-8736-929294a39990', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Fast language models, also known as efficient language models, have gained significant attention in recent years due to their importance in various applications. Here are some reasons why fast language models are crucial:\\n\\n1. **Real-time processing**: Fast language models enable real-time processing of natural language data, which is essential for many applications such as chatbots, virtual assistants, and language translation systems.\\n2. **Scalability**: Fast language models can process large volumes of data efficiently, making them suitable for big data applications, such as text analysis, sentiment analysis, and topic modeling.\\n3. **Improved accuracy**: Fast language models can be trained on large datasets, leading to improved accuracy and better performance in downstream tasks such as language translation, text summarization, and question answering.\\n4. **Faster inferencing**: Fast language models can perform fast inferencing, which is critical for applications that require rapid response times, such as speech recognition systems and language translation systems.\\n5. **Edge computing**: Fast language models can be deployed on edge devices, such as smartphones, smartwatches, and smart home devices, enabling real-time processing and reducing latency.\\n6. **Reduced computational resources**: Fast language models require less computational resources, such as memory and processing power, making them suitable for deployment on resource-constrained devices.\\n7. **Support for diverse applications**: Fast language models can be used in a wide range of applications, including but not limited to:\\n\\t* Text summarization\\n\\t* Language translation\\n\\t* Sentiment analysis\\n\\t* Question answering\\n\\t* Natural language processing (NLP)\\n\\t* Chatbots and virtual assistants\\n\\t* Language generation and writing\\n\\t* Speech recognition and speech synthesis\\n8. **Enhanced user experience**: Fast language models can provide a seamless user experience by enabling fast and accurate processing of natural language data, which is critical for applications that require real-time interaction.\\n9. **Advancements in AI research**: Fast language models have contributed significantly to advancements in artificial intelligence (AI) research, particularly in areas such as attention mechanisms and transformer architectures.\\n10. **Commercial applications**: Fast language models have numerous commercial applications, including but not limited to:\\n\\t* Customer service chatbots\\n\\t* Language translation software\\n\\t* Sentiment analysis tools\\n\\t* Content generation platforms\\n\\t* Search engines and recommendation systems\\n\\nIn summary, fast language models are important due to their ability to process natural language data efficiently, accurately, and in real-time, making them suitable for a wide range of applications that require high-performance language processing.', role='assistant', function_call=None, tool_calls=None))], created=1728501447, model='llama3-8b-8192', object='chat.completion', system_fingerprint='fp_6a6771ae9c', usage=CompletionUsage(completion_tokens=514, prompt_tokens=18, total_tokens=532, completion_time=0.428333333, prompt_time=0.002924508, queue_time=0.01064625, total_time=0.431257841), x_groq={'id': 'req_01j9sbdyx8e46st6y6wj7stqae'})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"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\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\",\n",
    ")\n",
    "\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-b0f17433-53ab-442b-93bb-be045b6385ba', choices=[Choice(finish_reason='stop', index=0, message=ChoiceMessage(role='assistant', content=\"Fast inference is crucial in various applications, particularly in deep learning and artificial intelligence (AI), due to several factors that drive the need for efficient and rapid processing of model predictions, classifications, or outputs. These applications often rely on complex models that consume significant computational resources and require fast response times to maintain or improve their functionality and user experience. Here are some key reasons why fast inference is important:\\n\\n1. **User Experience:** For applications that require real-time processing, such as chatbots, voice assistants, self-driving cars, or live video analytics, fast inference ensures a seamless and responsive experience. Users expect instant or near-instant feedback from these systems, making speed of inference critical.\\n\\n2. **Scalability:** As the size of deep learning models grows (with more layers, parameters, and complex architectures), their inference times increase. Fast inference is necessary to scale these models to large datasets and diverse use cases without compromising performance.\\n\\n3. **Real-time Applications:** Real-time applications such as surveillance systems, medical imaging analysis, and autonomous vehicles rely on fast inference to detect and respond to events or changes instantaneously.\\n\\n4. **Energy Efficiency:** Faster inference can lead to lower energy consumption by models, which is particularly relevant for embedded devices and edge computing. Reducing inference times can also help decrease the heat generated by these devices, extending their lifespan.\\n\\n5. **Competitiveness:** In competitive industries like finance, gaming, and e-commerce, being able to respond quickly can be a significant differentiator. For example, in trading systems, fast inference can make the difference between a profitable trade and a missed opportunity.\\n\\n6. **Data Processing:** In data-driven industries, such as customer analytics and personalized marketing, speed of inference is vital. It enables organizations to process data in real-time, analyze user behavior, and adjust their strategies accordingly to maximize engagement and customer satisfaction.\\n\\n7. **Autonomous Systems:** Self-driving cars, drones, and industrial robots rely on fast inference to make instant decisions based on sensor data, preventing accidents and improving efficiency.\\n\\nTo achieve fast inference, several strategies are employed, including:\\n\\n- **Model Pruning and Sparsification:** Techniques to reduce the model's size while preserving accuracy.\\n- **Knowledge Distillation:** Transfer knowledge from large models to smaller ones to speed up inference without sacrificing too much accuracy.\\n- **Quantization:** Converting floating-point numbers used in the model to lower-precision integers, reducing computational complexity and memory requirements.\\n- **Specialized Hardware:** The development of specialized chips, such as Graphical Processing Units (GPUs), Tensor Processing Units (TPUs), and Field-Programmable Gate Arrays (FPGAs), designed to accelerate AI computations.\\n- **Optimized Inference Libraries:** Utilizing libraries that are optimized for the hardware used, such as TensorFlow Lite, Core ML, and OpenVINO, to speed up model evaluation.\\n\\nOverall, the importance of fast inference lies in its ability to unlock real-time AI capabilities, enhance user experience, and improve business competitiveness across various industries.\", tool_calls=None), logprobs=None)], created=1728501518, model='llama3.1-8b', object='chat.completion', system_fingerprint='fp_97b75e13af', time_info=TimeInfo(completion_time=0.2908894339244663, prompt_time=0.0011948500755336617, queue_time=2.703e-05, total_time=0.2940342426300049, created=1728501518), usage=Usage(completion_tokens=610, prompt_tokens=16, total_tokens=626), service_tier=None)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from cerebras.cloud.sdk import Cerebras\n",
    "\n",
    "client = Cerebras(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=\"csk-e2e8kypw838rwmpjxd9nx2vn5jrertm339fnrcnt9c6p8hmx\"\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Why is fast inference important?\",\n",
    "        }\n",
    "],\n",
    "    model=\"llama3.1-8b\",\n",
    ")\n",
    "\n",
    "print(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
